{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpptSP4n1YGX"
      },
      "source": [
        "**Download libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laFoiGvnLj47",
        "outputId": "e882cc5c-f405-4dc7-c489-78a4f6137079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJjWStmBRozq"
      },
      "source": [
        "**RESNET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2t1y3ZGMy87",
        "outputId": "01affd97-60da-4eed-f4df-30a183835c7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 201MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25 loss: 1.3965370655059814\n",
            "Epoch 2/25 loss: 1.012216567993164\n",
            "Epoch 3/25 loss: 0.764329195022583\n",
            "Epoch 4/25 loss: 1.138136863708496\n",
            "Epoch 5/25 loss: 0.8742983341217041\n",
            "Epoch 6/25 loss: 0.74367356300354\n",
            "Epoch 7/25 loss: 0.8146423101425171\n",
            "Epoch 8/25 loss: 0.7722604274749756\n",
            "Epoch 9/25 loss: 0.7451421022415161\n",
            "Epoch 10/25 loss: 0.650442361831665\n",
            "Epoch 11/25 loss: 1.028049111366272\n",
            "Epoch 12/25 loss: 1.0574769973754883\n",
            "Epoch 13/25 loss: 1.239819049835205\n",
            "Epoch 14/25 loss: 0.8107497096061707\n",
            "Epoch 15/25 loss: 0.8404054045677185\n",
            "Epoch 16/25 loss: 0.625316321849823\n",
            "Epoch 17/25 loss: 0.5812388062477112\n",
            "Epoch 18/25 loss: 1.0164140462875366\n",
            "Epoch 19/25 loss: 0.808189868927002\n",
            "Epoch 20/25 loss: 0.9119093418121338\n",
            "Epoch 21/25 loss: 0.8127294778823853\n",
            "Epoch 22/25 loss: 0.9642001390457153\n",
            "Epoch 23/25 loss: 0.796804666519165\n",
            "Epoch 24/25 loss: 0.7294995784759521\n",
            "Epoch 25/25 loss: 0.5784299969673157\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained ResNet model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer to match the number of classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 5)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'resnet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_resnet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix_resnet.png')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wia_fT0grkMa"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKjXuIrGRwdf"
      },
      "source": [
        "**NASNetLarge**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3QCa-2CRyaf",
        "outputId": "27e24b9a-4842-4059-8612-63f2cc5ff7bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MNASNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=MNASNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth\" to /root/.cache/torch/hub/checkpoints/mnasnet1.0_top1_73.512-f206786ef8.pth\n",
            "100%|██████████| 16.9M/16.9M [00:00<00:00, 80.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained NASNet model\n",
        "model = models.mnasnet1_0(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 5)\n",
        "\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(331),  # NASNetLarge requires the input size to be 331x331\n",
        "    transforms.CenterCrop(331),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'nasnet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "class_names = image_datasets.classes\n",
        "true_labels_names = [class_names[label] for label in true_labels]\n",
        "pred_labels_names = [class_names[label] for label in pred_labels]\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels_names, pred_labels_names, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_nasnet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels_names, pred_labels_names)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.savefig('confusion_matrix_nasnet.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyzUuRhzxE6A"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZSLteQSXjxF"
      },
      "source": [
        "**SENET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lArISivqXmSy"
      },
      "outputs": [],
      "source": [
        "!pip install pretrainedmodels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCyuTr2WXoxJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pretrainedmodels\n",
        "\n",
        "\n",
        "import torch.hub\n",
        "model = torch.hub.load(\n",
        "    'moskomule/senet.pytorch',\n",
        "    'se_resnet50',\n",
        "    pretrained=True,)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 5)\n",
        "\n",
        "\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'senet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_senet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix SENet')\n",
        "plt.savefig('confusion_matrix_senet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHeQYfOM1GBa"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9LzH2N4YKZx"
      },
      "outputs": [],
      "source": [
        "!pip install efficientnet_pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_uSfSAnYu2f"
      },
      "source": [
        "**Efficient NET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nojarm7XYyLg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained EfficientNet model\n",
        "model_name = 'efficientnet-b0'  # You can increase the number for more complex models\n",
        "model = EfficientNet.from_pretrained(model_name)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model._fc = nn.Linear(model._fc.in_features, 5)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model._fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'efficientnet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_efficientnet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix EfficientNet')\n",
        "plt.savefig('confusion_matrix_efficientnet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMLLu_9M4vdx"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8KDQJGZZgMi"
      },
      "source": [
        "**MobileNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhL7ocWWZisZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained MobileNet model\n",
        "model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 5)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier[1].parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'mobilenet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_mobilenet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix MobileNet')\n",
        "plt.savefig('confusion_matrix_mobilenet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTkpGism7-Ik"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94R52IvnakDt"
      },
      "source": [
        "**Inception V3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3csh46ptanUC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assume your images are in a directory called 'dataset' with subdirectories for each emotion\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained Inception v3 model\n",
        "model = models.inception_v3(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layers to match the number of classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 5)\n",
        "model.AuxLogits.fc = nn.Linear(model.AuxLogits.fc.in_features, 5)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(299),\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.fc.parameters()},\n",
        "    {'params': model.AuxLogits.fc.parameters()}\n",
        "], lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Adjust model for training\n",
        "        model.train()\n",
        "        outputs, aux_outputs = model(inputs)\n",
        "        loss1 = criterion(outputs, labels)\n",
        "        loss2 = criterion(aux_outputs, labels)\n",
        "        loss = loss1 + 0.4*loss2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'inception_v3.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_inception.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix Inception v3')\n",
        "plt.savefig('confusion_matrix_inception.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19JXs8biMJME"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwc4FKfQcu1K"
      },
      "source": [
        "**DenseNET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDMUAj44cxIz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained DenseNet model\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 5)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'densenet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_densenet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix DenseNet')\n",
        "plt.savefig('confusion_matrix_densenet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtGUVm2DMKLV"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8bOEtQdBUN"
      },
      "source": [
        "**ResNetX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lScPVohxdDWb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained ResNeXt model\n",
        "model = models.resnext50_32x4d(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'resnext.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_resnext.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix ResNeXt')\n",
        "plt.savefig('confusion_matrix_resnext.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Wc-sGKMNqw"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF3qmRgjdXnk"
      },
      "source": [
        "**ShuffleNET**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn11CHIgda-9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained ShuffleNet model\n",
        "model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes = 5\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'shufflenet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_shufflenet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix ShuffleNet')\n",
        "plt.savefig('confusion_matrix_shufflenet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_CNDkgAMQkx"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQo2OizMeCpM"
      },
      "source": [
        "**Wide ResNet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYGPDZYkeILj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Dataset_aug'\n",
        "\n",
        "# Load a pre-trained Wide ResNet model\n",
        "model = models.wide_resnet50_2(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_classes = 5\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Set up data transformations\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "image_datasets = ImageFolder(data_dir, transform=data_transforms)\n",
        "\n",
        "# Determine the sizes of the training and validation sets\n",
        "train_size = int(0.8 * len(image_datasets))\n",
        "val_size = len(image_datasets) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(image_datasets, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} loss: {loss.item()}')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model, 'wide_resnet.pth')\n",
        "\n",
        "# After training, evaluate on the validation set\n",
        "model.eval()\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "\n",
        "# Save classification report to CSV\n",
        "df = pd.DataFrame(report).transpose()\n",
        "df.to_csv('classification_report_wide_resnet.csv')\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix Wide ResNet')\n",
        "plt.savefig('confusion_matrix_wide_resnet.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0FZ5XwzMVu4"
      },
      "outputs": [],
      "source": [
        "# First, store the class names in a variable\n",
        "class_names = image_datasets.classes\n",
        "\n",
        "# Then, print each class index with its corresponding name\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f'Index {i} corresponds to class: {class_name}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}